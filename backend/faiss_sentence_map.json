[{"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "SAM KINSON , NAVANEETH KRISHNAN C."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "VOC DATA SCIENCE\nREG."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "NO : THAXBOA049 , THAXBOA047\nTERM PAPER PRESENTATION\nDate : 23.02.2026"}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Unified Semantic Search for Multimodal Messages Using AI-Based Retrieval Models\nST."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "THOMAS COLLEGE (AUTONOMOUS), THRISSUR\nAffiliated to University of Calicut Nationally reaccredited with \u2018A++\u2019 Grade\n1\n1\nMr."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Rejin Varghese\nResearch Supervisor\n       Asst."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Professor & HOD\n    Dept."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "of Data Science\n2\nAbstract\nIntroduction\nDataset Description\nLiterature Review\nResult and Discussion\nConclusion\nOutput\nReferences\nOUTLINE\n3\nABSTRACT\nModern digital communication platforms enable users to exchange numerous voice notes and audio messages."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "However, current messaging systems mainly support keyword-based text search and lack semantic search within audio content."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "As a result, retrieving previously shared voice messages becomes difficult, especially when users do not remember the exact keywords."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "This study proposes an AI-based Intelligent Audio Semantic Search System for accurate voice note retrieval in conversational platforms."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The framework uses deep learning models such as DNN, RNN, LSTM, GRU, and HRNN to capture sequential patterns, contextual relationships, and hierarchical structures in audio data."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Audio signals are converted into features like MFCC and spectrograms, processed to extract meaningful representations, and mapped into a semantic embedding space to enable similarity-based audio retrieval."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "By integrating temporal modeling, hierarchical learning, and deep feature extraction, the system improves search accuracy, contextual relevance, and retrieval efficiency."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The proposed approach advances intelligent voice-based systems by enabling semantic audio search, offering significant improvements over traditional keyword-based retrieval methods and supporting next-generation voice-enabled communication platforms."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "INTRODUCTION\nThe rapid growth of digital communication platforms has significantly increased voice-based interactions through voice notes and audio messages."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "However, current messaging systems mainly support keyword-based text search and lack semantic understanding within audio content."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "As the volume of stored voice messages increases, retrieving specific audio information becomes difficult and time-consuming."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Traditional search mechanisms rely on exact keyword matching without contextual awareness."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "They cannot effectively analyze speech content, speaker characteristics, or contextual meaning inside voice messages, leading to inefficient retrieval of important audio data."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "To address this challenge, this study proposes an AI-based Intelligent Audio Semantic Search System."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The framework converts raw audio into feature representations such as MFCC and spectrograms and applies deep learning models (DNN, RNN, LSTM, GRU, HRNN) to capture sequential patterns, contextual relationships, and hierarchical temporal structures."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The extracted representations are mapped into a semantic vector space to enable similarity-based audio retrieval, improving search relevance, contextual accuracy, and retrieval efficiency beyond traditional keyword-based methods."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "4\nDATASET DESCRIPTION\nA."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Dataset Overview\nThe dataset is a multimodal communication dataset containing,voice notes and related metadata exchanged between users."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "It supports analysis of conversational behavior, speech recognition, and information retrieval."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Data Preprocessing and Feature Extraction\nThe dataset undergoes preprocessing to ensure consistency, reliability, and suitability for machine learning and deep learning models."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The major preprocessing and feature extraction steps include:\nSpeech Processing: Voice notes are converted into text using speech-to-text techniques."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Audio features such as MFCCs or speech embeddings are extracted, and confidence scores are assigned to evaluate recognition accuracy."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "5\nDataset Structure Overview\nDATASET DESCRIPTION\nVoice Message Attributes\n6\n7\nLITERATURE SURVEY\n1."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Deep Neural Network (DNN) [1, 2, 3, 4, 6, 7, 8, 11, 12]\nDeep Neural Networks (DNN) are widely used in audio processing and speech analysis because of their ability to learn complex patterns from raw audio signals."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "In audio-based systems, DNN models extract meaningful features such as MFCC (Mel-Frequency Cepstral Coefficients), spectrograms, and other frequency-domain representations to capture sound characteristics and acoustic patterns."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Proposed Usage Scenario\nStep 1: Feature Extraction:\n  The input audio query is converted into feature representations (e.g., MFCC or spectrogram)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 2: Feature Transformation\nThe DNN processes these features through multiple hidden layers\nWorking Principle in Audio\nAudio signals are first converted into feature representations such as MFCC or spectrograms."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "These extracted features are provided as input to the DNN model."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Multiple hidden layers learn hierarchical representations of sound patterns and voice characteristics."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The output layer performs classification or retrieves relevant audio content based on learned patterns."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "LITERATURE SURVEY\nStep 3: Embedding Generation\n The network identifies patterns related to voice, tone, and acoustic structure."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 4: Retrieval\nThe system retrieves the most relevant previously stored audio files based on similarity matching."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "A Deep Neural Network consists of an input layer, multiple hidden layers, and an output layer connected sequentially."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The input layer receives extracted feature vectors, which are passed through hidden layers where each neuron performs weighted summation followed by a non-linear activation function."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "As the data moves deeper through the network, it learns increasingly abstract and meaningful representations of the input features."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Finally, the output layer produces a compact embedding vector, and during training, errors are propagated backward to update the weights and improve performance."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "8\nLITERATURE SURVEY\n2."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Recurrent Neural Network (RNN) [1, 2, 3, 4, 6, 7, 8, 10, 11, 12]\nRecurrent Neural Networks (RNN) are specifically designed to handle sequential data such as audio signals and speech."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Unlike traditional DNN, RNN maintains a memory of previous inputs using hidden states, making it highly suitable for time-dependent audio processing tasks."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "In audio-based systems, RNN captures temporal dependencies in sound waves, helping the model understand patterns over time such as speech flow, tone variation, and acoustic transitions."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Working Principle in Audio\nAudio signals are converted into sequential feature representations (e.g., MFCC, spectrogram frames)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Each time step of the audio sequence is fed into the RNN."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The hidden state carries information from previous time steps."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The output is generated based on both current input and past contextual information."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "This sequential learning ability makes RNN more effective than traditional DNN for speech-based tasks."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Proposed Usage Scenario\n Step 1: Input Representation\n The query audio is converted into sequential feature frames."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 2: Sequential Processing\nThe RNN processes the audio frame-by-frame\n9\nLITERATURE SURVEY\nStep 3: Hidden State Update\nThe hidden states capture contextual voice information across time."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 4: Output Generation\nThe system retrieves the most relevant stored audio clips based on temporal similarity\nA Recurrent Neural Network processes sequential data one time step at a time while maintaining a hidden state that acts as memory."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "At each step, the network combines the current input with the previous hidden state to capture temporal dependencies."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "This allows the model to understand context in sequences such as speech or text."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "During training, errors are propagated backward through time to update weights and improve sequence prediction accuracy."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "10\nLITERATURE SURVEY\n3."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Hierarchical Recurrent Neural Network (HRNN) [3, 6, 10]\nHierarchical Recurrent Neural Networks (HRNNs) are an advanced form of RNN designed to model long sequential data by learning at multiple hierarchical levels."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Unlike standard RNNs that process sequences at a single level, HRNNs divide sequences into smaller segments and process them in stages."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "This hierarchical structure helps capture both short-term and long-term dependencies effectively."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "HRNNs are widely used in speech recognition."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "In multimodal information retrieval systems, HRNNs enable more effective contextual understanding by decomposing complex sequential data into structured temporal representations."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "By modeling fine-grained temporal patterns as well as broader contextual dynamics, HRNNs improve the system\u2019s ability to interpret semantic relationships across speech and text modalities."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "This hierarchical approach addresses the limitations of flat sequence models and enhances performance in tasks that require deep temporal reasoning."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Working Principle in Audio\nAudio signals are divided into small frames or segments (e.g., MFCC frames)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The lower-level RNN processes frame-level audio features."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The higher-level RNN captures long-term dependencies across segments."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The final output layer performs classification or retrieves relevant audio content."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "11\nLITERATURE SURVEY\nStep 3: Higher-Level RNN\nThe higher-level RNN captures long-term voice patterns and contextual meaning."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 4: Output Layer\nThe system retrieves the most relevant stored audio clips based on deep contextual similarity."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Proposed Usage Scenario\nStep 1: Input Segmentation\nThe input audio is divided into sequential frames."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 2: Lower-Level RNN\nThe lower-level RNN analyzes short-term acoustic features."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "A Hierarchical Recurrent Neural Network processes long sequences by dividing them into smaller segments and modeling them at multiple levels."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The lower-level RNN captures short-term patterns within each segment, while the higher-level RNN learns long-term dependencies across segments."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "This hierarchical structure improves memory efficiency and contextual understanding."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "During training, errors are propagated through both levels to optimize the model and enhance sequence prediction performance."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "12\nLITERATURE SURVEY\n4."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Long Short-Term Memory (LSTM) [1, 2, 3, 4, 6, 7, 8, 10, 11, 12]\nLong Short-Term Memory (LSTM) is a special type of Recurrent Neural Network designed to overcome the vanishing gradient problem in traditional RNNs."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "LSTM introduces memory cells and gating mechanisms to control the flow of information."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "It can capture long-term dependencies in sequential data more effectively than standard RNNs."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "LSTM is widely used in speech recognition, text prediction and machine translation."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "In multimodal information retrieval systems, LSTMs play a crucial role in modeling long-range temporal dependencies across speech, text, and visual modalities."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Their ability to maintain contextual continuity over extended sequences allows systems to generate more coherent and semantically meaningful outputs."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "As a result, LSTM-based architectures have significantly improved performance in tasks that require understanding temporal context and sequence structure."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Working Principle in Audio\nAudio signals are first converted into feature representations (e.g., MFCC or spectrogram)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "These sequential features are given as input to the LSTM network."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The forget gate removes irrelevant information from previous states."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The input gate updates the memory with new important audio features."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The output gate generates the hidden state for prediction or retrieval."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "13\nLITERATURE SURVEY\nStep 2: LSTM Cell Processing\nThe LSTM processes the sequence while maintaining long-term memory."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 3: Gate Mechanisms\nThe model captures contextual audio information."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 4: Output Generation\nThe system retrieves the most relevant stored audio clips based on learned temporal similarity."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Proposed Usage Scenario\nStep 1: Input Representation\nThe input audio is converted into sequential feature representations."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "An LSTM processes sequential data using a memory cell that maintains long-term information across time steps."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "At each step, three gates (forget, input, and output) regulate how information flows through the cell."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The cell state preserves important information while discarding irrelevant details, preventing vanishing gradients."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "During training, errors are propagated backward through time to update weights and improve sequence prediction accuracy."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "14\nLITERATURE SURVEY\n5.Gated Recurrent Unit (GRU)\nGated Recurrent Unit (GRU) is a simplified version of LSTM designed to handle sequential data and overcome the vanishing gradient problem."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Unlike LSTM, GRU combines the forget and input gates into a single update gate, making the architecture simpler and computationally efficient."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "It maintains long-term dependencies while reducing model complexity."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "GRU is widely used in speech processing, text generation, machine translation, and time-series prediction."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Working Principle in Audio\nAudio signals are converted into feature representations (e.g., MFCC or spectrogram)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "These sequential features are fed into the GRU network."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The reset gate determines how much past information to forget."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The update gate controls how much new information to store."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The hidden state is updated and used for prediction or retrieval."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Proposed Usage Scenario\nStep 1: Input Representation\nThe input audio is transformed into sequential feature vectors."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 2: GRU Cell Processing\nThe GRU processes the sequence while managing memory through update and reset gates."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "15\nLITERATURE SURVEY"}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 3: Gate Mechanisms\nThe model captures voice patterns and contextual audio features."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Step 4: Output Generation\nThe system retrieves the most relevant stored audio files based on similarity matching."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "A GRU processes sequential data using two gates that regulate information flow through the hidden state."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The update gate decides how much past information should be retained, while the reset gate controls how much past information should be forgotten."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The model combines previous memory with new candidate information to produce the final hidden state."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "During training, errors are propagated backward through time to update the weights and improve prediction accuracy."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "16\nRESULT AND DISCUSSION\n17\nCONCLUSION\nThis study focused on developing an intelligent audio search system capable of efficiently retrieving previously stored voice notes and audio clips, similar to advanced chat-based search functionalities."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Multiple deep learning models, including DNN,RNN, LSTM, GRU and HRNN, were implemented and evaluated using performance metrics such as Accuracy, Precision, Recall, and F1-Score to determine their effectiveness in audio content classification and retrieval."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "The experimental results demonstrate that deep learning architectures significantly improve audio search performance."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Among the evaluated models, HRNN achieved the highest performance with 97% accuracy."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Its hierarchical structure effectively captures both short-term acoustic patterns and long-term contextual dependencies in audio signals, making it highly suitable for complex audio retrieval tasks."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "RNN also delivered strong results, while LSTM showed comparatively lower performance in this specific audio-based application."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "GRU demonstrated strong performance with high accuracy and computational efficiency, effectively modeling sequential dependencies with a simpler gated architecture."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "In contrast, DNN achieved comparatively lower performance, as it lacks temporal memory mechanisms required for capturing sequential audio context."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "18\nOUTPUT\n19\nREFERENCES\nHarsh Ahlawat, Naveen Aggarwal, Deepti Gupta,Automatic Speech Recognition: A survey of deep learning techniques and approaches,International Journal of Cognitive Computing in Engineering,Volume6,2025,Pages,201237,ISSN26663074,https://doi.org/10.1016/j.ijcce.2024.12.007."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Vaishnavi and K."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Kumar, \"Speech-to-Text and Text-to-Speech Recognition Using Deep Learning,\" 2023 2nd International Conference on Edge Computing and Applications (ICECAA), Namakkal, India, 2023, pp."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "657-666, doi: 10.1109/ICECAA58104.2023.10212222."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Abdar et al., \"A Review of Deep Learning for Video Captioning,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2024.3522295."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Zhu, Cunjuan, Qi Jia, Wei Chen, Yanming Guo, and Yu Liu."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"Deep learning for video-text retrieval: a review.\" International Journal of Multimedia Information Retrieval 12, no."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Liao, Lizi, Le Hong Long, Zheng Zhang, Minlie Huang, and Tat-Seng Chua."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"MMConv: an environment for multimodal conversational search across multiple domains.\" In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pp."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "20\nREFERENCES\nIslam, Saiful, Aurpan Dash, Ashek Seum, Amir Hossain Raj, Tonmoy Hossain, and Faisal Muhammad Shah."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"Exploring video captioning techniques: A comprehensive survey on deep learning methods.\" SN Computer Science 2, no."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "2 (2021): 1-28."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Samleti, Sandeep, Ashish Mishra, Alok Jhajhria, Shivam Kumar Rai, and Gaurav Malik."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"Real Time Video Captioning Using Deep Learning.\" INTERNATIONAL JOURNAL OF ENGINEERING RESEARCH and TECHNOLOGY (IJERT) 10, no."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Singh Sengar, Abhiraj, Kritika Pandey, and Pragya Tewari."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"Image to Caption Generator Using Machine Learning and Deep Learning Models.\" Available at SSRN 5190843 (2025)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Hase, Dhanashri, Junaid Khan, Sahil Khot, Rehaan Qureshi, and F."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"WhatsApp Chat Analysis Based on NLP Using Machine Learning.\" International Journal of Innovative Research in Engineering (2023)."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Pellet-Rostaing, Arthur, Roxane Bertrand, Auriane Boudin, St\u00e9phane Rauzy, and Philippe Blache."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"A multimodal approach for modeling engagement in conversation.\" Frontiers in Computer Science 5 (2023): 1062342."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "21\nREFERENCES\n T."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Ambikairajah, \"A Comprehensive Review of Speech Emotion Recognition Systems,\" in IEEE Access, vol."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "47795-47814, 2021, doi: 10.1109/ACCESS.2021.3068045."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Lieskovsk\u00e1, Eva, Maro\u0161 Jakubec, Roman Jarina, and Michal Chmul\u00edk."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"A Review on Speech Emotion Recognition Using Deep Learning and Attention Mechanism\" Electronics 10, no."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "https://doi.org/10.3390/electronics10101163\nWang, Yiyu, Jungang Xu, and Yingfei Sun."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\u201cEnd-to-End Transformer Based Model for Image Captioning.\u201d Proceedings of the AAAI Conference on Artificial Intelligence 36 (3): 2585\u201394."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "https://doi.org/10.1609/aaai.v36i3.20160."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Ju, Yeong-Joon, Ho-Joong Kim, and Seong-Whan Lee."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"MIRe: Enhancing multimodal queries representation via fusion-free modality interaction for multimodal retrieval.\" In Findings of the Association for Computational Linguistics: ACL 2025, pp."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "Lin, Sheng-Chieh, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping."}, {"message_id": "73329a8a-d339-4f9f-af1e-17c042961d1d", "sentence": "\"Mm-embed: Universal multimodal retrieval with multimodal llms, 2024.\" URL https://org/abs/2411.02571."}]